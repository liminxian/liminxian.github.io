
<html><head>
<title>Xiangbo Shu's Homepage</title>
<style type="text/css">
body {
	margin-top: 30px;
	margin-bottom: 30px;
	margin-left: 100px;
	margin-right: 100px;
}
p {
	margin-top: 0px;
	margin-bottom: 0px;
}

.caption {
	font-size: 34px;
	font-weight: normal;
	color: #000;
	font-family: Constantia, "Lucida Bright", "DejaVu Serif", Georgia, serif;
}
.caption-1 {
	font-size: 16px;
	font-family: Tahoma, Geneva, sans-serif;
}
.caption-2 {
	font-size: 16px;
	font-family: Tahoma, Geneva, sans-serif;
	font-weight: bold;
	color: #990000;
}
.caption-3 {
	font-size: 16px;
	font-family: Tahoma, Geneva, sans-serif;
	font-weight: bold;
	color: #F00;
}

.caption-4 {
	font-size: 16px;
	font-family: Tahoma, Geneva, sans-serif;
	color: #990000;
}
.content {
	font-size: 16px;
	font-family: Tahoma, Geneva, sans-serif;
	text-align: justify;
}
.content a {
	font-size: 16px;
	font-family: Tahoma, Geneva, sans-serif;
	color: #000;
}
.content strong a {
	font-size: 16px;
	font-family: Tahoma, Geneva, sans-serif;
	color: #990000;
}
.title-small {
	font-size: 20px;
	font-family: Georgia, "Times New Roman", Times, serif;
	font-weight: bold;
	color: #F90;
}
.title-large {
	font-size: 28px;
	font-family: Georgia, "Times New Roman", Times, serif;
	font-weight: bold;
	color: #000;
}
.margin {
	font-size: 10px;
	line-height: 10px;
}
.margin-small {
	font-size: 5px;
	line-height: 5px;
}
.margin-large {
	font-size: 16px;
	line-height: 16px;
}
a:link {
	text-decoration: none;
}
a:visited {
	text-decoration: none;
}
content a:link {
	text-decoration: none;
}
content a:visited {
	text-decoration: none;
}
a:hover {
	text-decoration: underline;
}
a:active {
	text-decoration: underline;
	color: #000000;
	font-family: Tahoma, Geneva, sans-serif;
}
strong a:active {
	text-decoration: underline;
	color: #000000;
}
img
{
 border-color: black;
}


</style>
<meta http-equiv="Content-Type" content="text/html; charset=gbk"></head>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-53682931-1', 'auto');
  ga('send', 'pageview');

</script>

<body>

<table border="0" width="100%">
  <tbody>

    <tr>

    <td width="300"><table>
			<tr><td>
			<a href="images/xiangboshu.jpg"><img src="images/xiangboshu.jpg"  height="270" alt="Xiangbo Shu photo" border=1></td></tr>
			</table></td>
    <td width="15"></td>
    <td></td>
    <td><table border="0" width="100%">
      <tbody><tr height="20">
        <td colspan="2"></td></tr>


         <tr height="60">
        <td>
           <p class="caption">Xiangbosss Shu<br><br></p>
           <p class="content">Assistant Professor, Ph.D.</p>
            <p class="content">Intelligent Media Analysis Group (IMAG)</p>
           <p class="content">School of Computer Science and Engineering</p>
            <p class="content">Nanjing University of Science and Technology</p>
        </td>
      </tr>

      <tr height="60">
        <td><table border="0" width="100%">
          <tbody><tr height="20">
            <td width="55">

              <p class="content"><strong>Email: </strong></p><br></td>
            <td>
              <p class="content">shuxb [AT] njust [DOT] edu [DOT] cn</p><br></td>
          </tr>

        </tbody></table></td>
      </tr>

      <tr height="40">
        <td>
          <p class="margin">&nbsp;</p>
          <p class="content"><strong><a href="#sect-software">Software</a></strong> | <strong><a href="http://scholar.google.com/citations?user=UdpacsMAAAAJ&hl=en">Google Scholar</a></strong> | <strong><a href="https://github.com/junyanz/">GitHub</a></strong> |  <strong><a href="https://arxiv.org/a/zhu_j_5.html">Arxiv</a></strong></p>
					<p class="content"><strong><a href="#sect-publications">Papers</a></strong> | <strong><a href="#sect-talks">Talks</a></strong> | <strong><a href="#sect-awards">Awards</a></strong> | <strong><a href="pdf/thesis_highres.pdf"> Thesis </a></strong> </p>
        </td>
      </tr>
      <tr height="20">
        <td colspan="2"></td></tr>
    </tbody></table></td>
  </tr>
</tbody></table>
<p class="margin">&nbsp;</p>

<table border="0" >
  <tbody>
    <tr>
      <td width="900"> <p align="justify" class="content">I am currently  an Assistant Professor in <strong><a href="http://web.mit.edu/torralba/www/" target="_blank" rel="nofollow" class="caption-2">Intelligent Media Analysis Group (IMAG)</a></strong>, Prof. <strong><a href="https://billf.mit.edu/" target="_blank" rel="nofollow" class="caption-2">William T. Freeman</a></strong>, and Prof. <strong><a href="http://web.mit.edu/cocosci/josh.html" target="_blank" rel="nofollow" class="caption-2">Josh Tenenbaum</a></strong>. I obtained my Ph.D. from UC Berkeley after spending five wonderful years at CMU and Berkeley with Prof. <strong><a href="http://www.eecs.berkeley.edu/~efros/" target="_blank" rel="nofollow" class="caption-2">Alexei A. Efros</a></strong>. I study computer vision, computer graphics, and machine learning with a goal of building machines capable of understanding and recreating our visual world.</p>
				<br>
				<p align="justify" class="content"> My Ph.D. was supported by a <strong><a href="https://research.fb.com/fellows/zhu-jun-yan/" target="_blank" rel="nofollow" class="caption-2">Facebook Fellowship</a></strong>.
					My dissertation won the 2018 ACM SIGGRAPH Outstanding Doctoral Dissertation <strong><a href="https://www.siggraph.org/outstanding-doctoral-dissertation-award-jun-yan-zhu" target="_blank" rel="nofollow" class="caption-2">Award</a></strong> from SIGGRAPH
					and 2017-18 David J. Sakrison Memorial <strong><a href="https://www2.eecs.berkeley.edu/Students/Awards/17/" target="_blank" rel="nofollow" class="caption-2">Prize</a></strong> for outstanding doctoral research from the UC Berkeley EECS Department.</p>
        <br>

    <p align="justify" class="content">I received my B.E in Computer Sciences from Tsinghua University in 2012, where I worked with Prof. <strong><a href="http://pages.ucsd.edu/~ztu/" target="_blank" rel="nofollow" class="caption-2">Zhuowen Tu</a></strong> and Dr. <strong><a href="http://research.microsoft.com/en-us/people/echang/" target="_blank" rel="nofollow" class="caption-2">Eric Chang</a></strong> at Microsoft Research Asia and worked with
			Prof. <strong><a href="http://cg.cs.tsinghua.edu.cn/" target="_blank" rel="nofollow" class="caption-2">Shi-Min Hu</a></strong> at Tsinghua's Graphics Group.</p>
    </tr>
  </tbody>
</table>

<br>

<br>
<br>

<p id="sect-events" class="title-large">News &amp; Events</p>
<p class="content">PyTorch <strong> <a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix">code</a></strong> for CycleGAN and pix2pix (with PyTorch 0.4+).</p>
<p class="content">CycleGAN/pix2pix <strong><a href="https://blog.udacity.com/2018/08/deep-learning-udacity-evolves.html">lecture</a></strong> at Udacity deep learning course.</p>
<p class="content">SIGGRAPH Asia 2018, Technical Papers Committee member.<p>
<p class="content">CVPR 2018 <strong><a href="https://sites.google.com/view/cvpr2018tutorialongans/">Tutorial</a></strong> on Generative Adversarial Networks.</p>
<p class="content">ICCV 2017 <strong><a href="https://sites.google.com/view/iccv-2017-gans/">Tutorial</a></strong> on Generative Adversarial Networks.</p>
<p class="content">ICML 2017 <strong><a href="http://icmlviz.github.io/">Workshop</a></strong>  on Visualization for Deep Learning.</p>
<p class="content">SIGGRAPH Asia 2014 invited <strong><a href="http://kevinkaixu.net/courses/ddvc.html">Course</a></strong> on Data-Driven Visual Computing.</p>
<br>


<p id="sect-publications" class="title-large">Publications</p>



<table border="0">
  <tbody><tr>
    <td width="140"><a href="https://tcwang0509.github.io/pix2pixHD/"><img src="imgs/vid2vid.gif" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="800"><p class="content"><strong>Video-to-Video Synthesis</strong></p>
      <p class="content"><a href="https://tcwang0509.github.io/">Ting-Chun Wang</a>, <a href="http://mingyuliu.net/">Ming-Yu Liu</a>, Jun-Yan Zhu, <a href="https://liuguilin1225.github.io/">Guilin Liu</a>, Andrew Tao, <a href="http://jankautz.com/">Jan Kautz</a>, <a href="http://catanzaro.name/">Bryan Catanzaro</a></p>
      <p class="content">In arXiv, 2018</p>
	   <p class="margin-small">&nbsp;</p>
	   <p class="content">
      <strong><a href="https://tcwang0509.github.io/vid2vid/">Project</a></strong> |
			<strong> <a href="https://github.com/NVIDIA/vid2vid"> Code </a></strong> |
			<strong><a href="https://tcwang0509.github.io/vid2vid/paper_vid2vid.pdf">Full Paper</a></strong> |
			<strong><a href="https://arxiv.org/abs/1808.06601">arXiv</a></strong> |
		  <strong><a href="https://youtu.be/S1OwOd-war8">Youtube</a> </strong> |
			<strong><a href="https://tcwang0509.github.io/vid2vid/Bibtex.txt">BibTex</a></strong> <br> </p>  </tr>
</tbody></table>

<table border="0">
  <tbody><tr>
    <td width="140"><img src="imgs/cycada.jpg" border="1"width="210"></td>
    <td width="20"></td>
    <td valign="middle" width="800">
			<p class="content"><strong> CyCADA: Cycle-Consistent Adversarial Domain Adaptation</strong></p>
      <p class="content"><a href="https://people.eecs.berkeley.edu/~jhoffman/">Judy Hoffman</a>, <a href="https://github.com/erictzeng">Eric Tzeng</a>, <a href="https://taesung.me/">Taesung Park</a>, Jun-Yan Zhu, <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>, <a href="http://www.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>, and <a href="http://people.eecs.berkeley.edu/~pathak/">Trevor Darrell</a></p>
      <p class="content">In International Conference on Machine Learning (<strong>ICML</strong>), 2018</p>
	 <p class="margin-small">&nbsp;</p>
	   <p class="content">
     <strong><a href="https://arxiv.org/abs/1711.03213">Paper</a></strong> |
		 <strong><a href="https://github.com/jhoffman/cycada_release">Code</a></strong> |
			<strong><a href="projects/cycada/cycada.txt">BibTex</a></strong> <br>
		</p>
      </tr>
</tbody></table>

<table border="0">
  <tbody><tr>
    <td width="140"><a href="https://tcwang0509.github.io/pix2pixHD/"><img src="imgs/pix2pixHD.gif" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="800"><p class="content"><strong>High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs</strong></p>
      <p class="content"><a href="https://tcwang0509.github.io/">Ting-Chun Wang</a>, <a href="http://mingyuliu.net/">Ming-Yu Liu</a>, Jun-Yan Zhu, Andrew Tao, <a href="http://jankautz.com/">Jan Kautz</a>, <a href="http://catanzaro.name/">Bryan Catanzaro</a><p>
      <p class="content">In IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2018</p>
				<p class="content">See GTC 2018 <strong> <a href="https://youtu.be/95nphvtVf34?t=1h36m15s">Keynote</a></strong>.</p>
	 <p class="margin-small">&nbsp;</p>
	   <p class="content">
      <strong><a href="https://tcwang0509.github.io/pix2pixHD/">Project</a></strong>
			| <strong> <a href="https://github.com/NVIDIA/pix2pixHD"> Code </a></strong>
			| <strong><a href="https://arxiv.org/abs/1711.11585">Paper</a></strong>
			| <strong> <a href="https://youtu.be/3AIpPlzM_qs">Youtube</a> </strong>
			| <strong> <a href="https://tcwang0509.github.io/papers/CVPR18/pix2pixHD_slides.pptx">Slides</a> </strong>
			| <strong><a href="https://tcwang0509.github.io/pix2pixHD/Bibtex.txt">BibTex</a></strong><br> </p>
      </tr>
</tbody></table>


<table border="0">
  <tbody><tr>
    <td width="140"><img src="imgs/stAdv.jpg" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="800"><p class="content"><strong>Spatially Transformed Adversarial Examples</strong></p>
      <p class="content">Chaowei Xiao*, Jun-Yan Zhu*, <a href="http://bli89.web.engr.illinois.edu/">Bo Li</a>, <a href="http://web.eecs.umich.edu/~mingyan/">Mingyan Liu</a>, and <a href="https://people.eecs.berkeley.edu/~dawnsong/">Dawn Song</a></p>
      <p class="content">In International Conference on Learning Representations (<strong>ICLR</strong>), 2018</p>
				<p class="margin-small">&nbsp;</p>
		 	   <p class="content">
		      <strong><a href="https://arxiv.org/abs/1801.02612">Paper</a></strong> |
		 			<strong><a href="projects/stAdv/stAdv.bib">BibTex</a></strong><br></p>    </tr>
</tbody></table>


<table border="0">
  <tbody><tr>
    <td width="140"><img src="imgs/advGAN.jpg" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="800"><p class="content"><strong>Generating Adversarial Examples with Adversarial Networks</strong></p>
      <p class="content">Chaowei Xiao, <a href="http://bli89.web.engr.illinois.edu/">Bo Li</a>, Jun-Yan Zhu, <a href="http://web.eecs.umich.edu/~mingyan/">Mingyan Liu</a>, and <a href="https://people.eecs.berkeley.edu/~dawnsong/">Dawn Song</a></p>
      <p class="content">In International Joint Conference on Artificial Intelligence (<strong>IJCAI</strong>), 2018</p>
				<p class="margin-small">&nbsp;</p>
		 	   <p class="content">
		      <strong><a href="https://arxiv.org/abs/1801.02610">Paper</a></strong> |
		 			<strong><a href="projects/advGAN/advGAN.bib">BibTex</a></strong><br></p>    </tr>
</tbody></table>

<table border="0">
  <tbody><tr>
    <td width="140"><a href="https://junyanz.github.io/BicycleGAN/"><img src="imgs/BicycleGAN.gif" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="800"><p class="content"><strong>Toward Multimodal Image-to-Image Translation</strong></p>
      <p class="content">Jun-Yan Zhu, <a href="https://richzhang.github.io/">Richard Zhang</a>, <a href="http://people.eecs.berkeley.edu/~pathak/">Deepak Pathak</a>, <a href="http://people.eecs.berkeley.edu/~pathak/">Trevor Darrell</a>, <a href="http://www.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>, <a href="http://www.oliverwang.info/">Oliver Wang</a>, and <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a></p>
      <p class="content">In Advances in Neural Information Processing Systems (<strong>NIPS</strong>), 2017</p>
	 <p class="margin-small">&nbsp;</p>
	   <p class="content">
      <strong><a href="https://junyanz.github.io/BicycleGAN/">Project</a></strong> | <strong><a href="https://github.com/junyanz/BicycleGAN">Code</a></strong> | <strong><a href="https://arxiv.org/abs/1711.11586">Paper</a></strong> | <strong><a href="https://youtu.be/JvGysD2EFhw">Youtube</a></strong> | <strong><a href="https://junyanz.github.io/BicycleGAN/index_files/poster_nips_v3.pdf">Poster</a></strong> |
			<strong><a href="https://junyanz.github.io/BicycleGAN/BicycleGAN.txt">BibTex</a></strong><br> </p>
      </tr>
</tbody></table>

<table border="0">
  <tbody><tr>
    <td width="140"><a href="https://junyanz.github.io/CycleGAN/"><img src="imgs/CycleGAN.jpg" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="800"><p class="content"><strong>Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</strong></p>
      <p class="content">Jun-Yan Zhu*, <a href="https://taesung.me/">Taesung Park</a>*, <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>, and <a href="http://www.eecs.berkeley.edu/~efros/">Alexei A. Efros</a></p>
      <p class="content">In IEEE International Conference on Computer Vision (<strong>ICCV</strong>), 2017</p>
				<p class="content">Mentioned in NY Times <strong><a href="https://www.nytimes.com/interactive/2018/01/02/technology/ai-generated-photos.html">article</a></strong> on GANs.</p>

	 <p class="margin-small">&nbsp;</p>
	   <p class="content">
      <strong><a href="https://junyanz.github.io/CycleGAN/">Project</a></strong> |
			<strong><a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix">PyTorch</a></strong> |
			<strong><a href="https://github.com/junyanz/CycleGAN">Torch</a></strong> |
			<strong><a href="https://arxiv.org/pdf/1703.10593.pdf">Paper</a></strong> <br>
			<strong><a href="https://youtu.be/AxrKVfjSBiA">Spotlight Talk</a></strong> |
			<strong><a href="talks/image_translation.pptx">Slides</a></strong> |
			<strong><a href="https://junyanz.github.io/CycleGAN/CycleGAN.txt">BibTex</a></strong><br> </p>
      </tr>
</tbody></table>



<table border="0">
  <tbody><tr>
    <td width="140"><a href="https://phillipi.github.io/pix2pix/"><img src="imgs/pix2pix.jpg" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="800"><p class="content"><strong>Image-to-Image Translation with Conditional Adversarial Nets</strong></p>
      <p class="content"><a href="http://web.mit.edu/phillipi/">Phillip Isola</a>, Jun-Yan Zhu, <a href="https://people.eecs.berkeley.edu/~tinghuiz/">Tinghui Zhou</a>, and <a href="http://www.eecs.berkeley.edu/~efros/">Alexei A. Efros</a></p>
      <p class="content">In IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2017</p>
			<p class="content">See Distill <strong> <a href="https://distill.pub/2017/aia/">blog</a> </strong> and the Economist <strong><a href="https://www.economist.com/science-and-technology/2017/07/01/fake-news-you-aint-seen-nothing-yet">article</a></strong>  | Also see neat uses of <strong> <a href="https://twitter.com/search?vertical=default&q=pix2pix&src=typd">#pix2pix</a> </strong> on Twitter.</p>
	   <p class="content">
      <strong><a href="https://phillipi.github.io/pix2pix/">Project</a></strong> |
			<strong><a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix">PyTorch</a></strong> |
			<strong><a href="https://github.com/phillipi/pix2pix">Torch</a></strong> |
			<strong><a href="https://arxiv.org/pdf/1611.07004.pdf">Paper</a></strong> |
		  <strong><a href="talks/image_translation.pptx">Slides</a></strong> |
      <strong><a href="projects/pix2pix/pix2pix.bib">BibTex</a></strong><br></p>
      </tr>
</tbody></table>

<p class="margin">&nbsp;</p>
<table border="0">
  <tbody><tr>
    <td width="140"><a href="imgs/icolor_demo.gif"><img src="imgs/icolor_demo.gif" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="800">
			<p class="content"><strong>Real-Time User-Guided Image Colorization with Learned Deep Priors</strong></p>
      <p class="content"><a href="http://richzhang.github.io/">Richard Zhang</a>*, Jun-Yan Zhu*, <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>, <a href="http://young-geng.xyz/">Xinyang Geng</a>, Angela S. Lin, Tianhe Yu, and <a href="http://www.eecs.berkeley.edu/~efros/">Alexei A. Efros</a></p>
      <p class="content">In ACM Transactions on Graphics (<strong>SIGGRAPH</strong>), 2017</p>

			<p class="content">
			<strong><a href="https://richzhang.github.io/ideepcolor/">Project</a></strong> |
			<strong><a href="https://github.com/junyanz/interactive-deep-colorization">UI Code</a></strong> |
			<strong><a href="https://github.com/richzhang/colorization-pytorch">PyTorch Training</a></strong> |
			<strong><a href="https://youtu.be/eL5ilZgM89Q">Youtube</a></strong> |
			<strong><a href="projects/ideepcolor/iColor_release.mp4">Video</a></strong> <br>
			<strong><a href="https://arxiv.org/abs/1705.02999">Paper</a></strong> |
			<strong><a href="talks/ideepcolor.pptx">Slides</a></strong> |
		  <strong><a href="https://youtu.be/rp5LUSbdsys">Talk<a></strong> |
      <strong><a href="projects/ideepcolor/ideepcolor.txt">BibTex</a></strong> |
			<strong><a href="https://youtu.be/eiFzQI7LzO0?t=5690">Fastforward<a></strong> <br>
			</p>
      </tr>
</tbody></table>

<table border="0">
  <tbody><tr>
    <td width="140"><a href=""><img src="imgs/lfv.jpg" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="800"><p class="content"><strong>Light Field Video Capture Using a Learning-Based Hybrid Imaging System</strong></p>
			<p class="content"><a href="https://tcwang0509.github.io/">Ting-Chun Wang</a>, Jun-Yan Zhu, <a href="http://nkhademi.com/">Nima Khademi Kalantari</a>, <a href="http://www.eecs.berkeley.edu/~efros/"> Alexei A. Efros</a>, and <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a></p>
        <p class="content">In ACM Transactions on Graphics (<strong>SIGGRAPH</strong>), 2017</p>
				<p class="content">
				<strong> <a href="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/SIG17/lfv/">Project</a></strong> |
				<strong> <a href="https://github.com/junyanz/light-field-video">GitHub</a></strong> |
			  <strong> <a href="https://youtu.be/TqVKcssYfAo">Youtube</a></strong> |
				<strong> <a href="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/SIG17/lfv/paperData/LF_video_Code_v1.1.zip">Training code</a></strong> <br>
				<strong> <a href="https://arxiv.org/abs/1705.02997">Paper</a></strong> |
				<strong><a href="https://youtu.be/s3hAqdNejXQ">Talk<a></strong> |
			  <strong> <a href="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/SIG17/lfv/paperData/LF_video.mp4">Video<a></strong> |
				<strong> <a href="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/SIG17/lfv/paperData/LF_video_Dataset.zip">Data (18GB)<a></strong> |
		    <strong> <a href="projects/lfv/lfv.txt">BibTex</a> </strong><br></p>
      </tr>
</tbody></table>

<p class="margin">&nbsp;</p>
<table border="0">
  <tbody><tr>
    <td width="140"><a href="http://efrosgans.eecs.berkeley.edu/iGAN/"><img src="imgs/eccv16_igan.jpg" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="800"><p class="content"><strong>Generative Visual Manipulation on the Natural Image Manifold</strong></p>
      <p class="content">Jun-Yan Zhu, <a href="http://www.philkr.net/">Philipp Kr&auml;henb&uuml;hl</a>, <a href="http://www.adobe.com/technology/people/seattle/eli-shechtman.html">Eli Shechtman</a>, and <a href="http://www.eecs.berkeley.edu/~efros/">Alexei A. Efros</a></p>
      <p class="content">In European Conference on Computer Vision (<strong>ECCV</strong>), 2016</p>
			<p class="content">See Distill <strong> <a href="https://distill.pub/2017/aia/">blog</a> </strong> and <strong> <a href="http://alumni.berkeley.edu/california-magazine/winter-2016-reality-bites/paint-numbers-algorithms-artistically-challenged">article</a> </strong> in California Magazine
      <p class="margin-small">&nbsp;</p>
	   <p class="content">
      <strong><a href="http://efrosgans.eecs.berkeley.edu/iGAN/">Project</a> </strong> |
			<strong><a href="https://youtu.be/9c4z6YsBGQ0">YouTube</a> </strong> |
			<strong><a href="https://github.com/junyanz/iGAN">GitHub</a>  </strong> |
			<strong><a href="https://arxiv.org/abs/1609.03552">Paper</a></strong><br>
      <strong><a href="http://efrosgans.eecs.berkeley.edu/iGAN/slides.pptx"> Slides </a> </strong> |
			<strong><a href="http://efrosgans.eecs.berkeley.edu/iGAN/video.mp4"> Video </a> </strong> |
			 <strong><a href="http://efrosgans.eecs.berkeley.edu/iGAN/eccv16_iGAN.bib"> BibTex </a></strong><br></p>
      </tr>
</tbody></table>

<p class="margin">&nbsp;</p>
<table border="0">
  <tbody><tr>
    <td width="140"><a href="projects/lfmr/eccv16_lfmr.pdf"><img src="imgs/eccv16_lfmr.jpg" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="800"><p class="content"><strong>A 4D Light-Field Dataset and CNN Architectures for Material Recognition</strong></p>
      <p class="content"><a href="https://tcwang0509.github.io/">Ting-Chun Wang</a>, Jun-Yan Zhu, Ebi Hiroaki, <a href="http://www.nec-labs.com/~manu/">Manmohan Chandraker</a>, <a href="http://www.eecs.berkeley.edu/~efros/"> Alexei A. Efros</a>, and <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a></p>
      <p class="content">In European Conference on Computer Vision (<strong>ECCV</strong>), 2016</p>
      <p class="margin-small">&nbsp;</p>
     <p class="content">
      <strong><a href="projects/lfmr/eccv16_lfmr.pdf">Paper</a></strong> |
			<strong><a href="https://tcwang0509.github.io/papers/ECCV16/dataset.html">Data (thumbnail)</a></strong> |
			<strong><a href="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV16/LF_dataset.zip">Full data (15.9G)</a></strong> <br>
      <strong><a href ="https://tcwang0509.github.io/papers/ECCV16/full_scene.html">Supplement</a></strong> |
			<strong><a href="projects/lfmr/lfmr_poster.pdf">Poster</a> </strong> |
			<strong> <a href="projects/lfmr/eccv16_lfmr.bib">BibTex</a></strong> <br></p>
      </tr>
</tbody></table>



<p class="margin">&nbsp;</p>
<table border="0">
  <tbody><tr>
    <td width="140"><a href="http://efrosprojects.eecs.berkeley.edu/realism/index.html"><img src="imgs/iccv15_realism.jpg" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="800"><p class="content"><strong>Learning a Discriminative Model for the Perception of Realism in Composite Images</strong></p>
      <p class="content">Jun-Yan Zhu, <a href="http://www.philkr.net/">Philipp Kr&auml;henb&uuml;hl</a>, <a href="http://www.adobe.com/technology/people/seattle/eli-shechtman.html">Eli Shechtman</a>, and <a href="http://www.eecs.berkeley.edu/~efros/">Alexei A. Efros</a></p>
      <p class="content">In IEEE International Conference on Computer Vision (<strong>ICCV</strong>), 2015</p>
      <p class="margin-small">&nbsp;</p>
	   <p class="content">
      <strong><a href="http://efrosprojects.eecs.berkeley.edu/realism/index.html">Project</a></strong> |
			<strong><a href="http://efrosprojects.eecs.berkeley.edu/realism/iccv15_realism.pdf">Paper</a> </strong> |
			<strong><a href="https://github.com/junyanz/RealismCNN">GitHub</a></strong> |
	   <strong> <a href ="http://efrosprojects.eecs.berkeley.edu/realism/realism_slides.pptx"> Slides </a> </strong> |
			 <strong><a href="http://efrosprojects.eecs.berkeley.edu/realism/realism_poster.pdf"> Poster </a></strong> |
				 <strong><a href="http://efrosprojects.eecs.berkeley.edu/realism/iccv15_realism.bib"> BibTex </a> <br></p>
      </tr>
</tbody></table>

<p class="margin">&nbsp;</p>
<table border="0">
  <tbody><tr>
    <td width="140"><a href="http://efrosprojects.eecs.berkeley.edu/mirrormirror/index.html"><img src="imgs/mirrormirror.jpg" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="800"><p class="content"><strong>Mirror Mirror: Crowdsourcing Better Portraits</strong></p>
      <p class="content">Jun-Yan Zhu, <a href="http://www.agarwala.org/index.html">Aseem Agarwala</a>, <a href="http://www.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>, <a href="http://www.adobe.com/technology/people/seattle/eli-shechtman.html">Eli Shechtman</a>, and <a href="http://www.juew.org/"> Jue Wang</a></p>
      <p class="content">In ACM Transactions on Graphics (<strong>SIGGRAPH Asia</strong>), 2014</p>
      <p class="margin-small">&nbsp;</p>
	   <p class="content">
      <strong><a href="http://efrosprojects.eecs.berkeley.edu/mirrormirror/index.html">Project (code)</a></strong> |
			<strong> <a href="http://efrosprojects.eecs.berkeley.edu/mirrormirror/mirrormirror_small.pdf">Paper</a></strong> |
			<strong><a href="http://efrosprojects.eecs.berkeley.edu/mirrormirror/data.zip">Data</a></strong> |
	    <strong><a href="http://efrosprojects.eecs.berkeley.edu/mirrormirror/mirrormirror_slides.pptx">Slides</a></strong> |
	     <strong><a href="http://efrosprojects.eecs.berkeley.edu/mirrormirror/mirrormirror_supp.pdf">Supplement</a></strong> |
	     <strong><a href="http://efrosprojects.eecs.berkeley.edu/mirrormirror/mirrormirror.bib">BibTex</a></strong><br></p>
      </tr>
</tbody></table>

<table border="0">
  <tbody><tr>
    <td width="140"><a href="http://efrosprojects.eecs.berkeley.edu/averageExplorer/index.html"><img src="imgs/sig14_cat_small.gif" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="800"><p class="content"><strong>AverageExplorer: Interactive Exploration and Alignment of Visual Data Collections</strong></p>
      <p class="content">Jun-Yan Zhu, <a href="http://www.cs.ucdavis.edu/~yjlee/">Yong Jae Lee</a> and <a href="http://www.eecs.berkeley.edu/~efros/">Alexei A. Efros</a></p>
      <p class="content">In ACM Transactions on Graphics (<strong>SIGGRAPH</strong>), 2014</p>
      <p class="margin-small">&nbsp;</p>
       <p class="content">See <strong> <a href="http://www.newyorker.com/tech/elements/out-of-many-one">article</a> </strong> in The New Yorker
      <p class="content"><strong> <a href="http://efrosprojects.eecs.berkeley.edu/averageExplorer/index.html">Project</a> </strong>
      <strong><a href="http://efrosprojects.eecs.berkeley.edu/averageExplorer/averageExplorer.pdf"></a></strong> |
      <strong><a href="http://youtu.be/1QgL_aPPCpM">YouTube</a></strong> |
			<strong><a href="http://efrosprojects.eecs.berkeley.edu/averageExplorer/averageExplorer.pdf"> Paper</a></strong> |
        <strong> <a href="http://efrosprojects.eecs.berkeley.edu/averageExplorer/averageExplorer_slides.pptx">Slides</a></strong> |
        <strong><a href="http://efrosprojects.eecs.berkeley.edu/averageExplorer/averageExplorer_supp.pdf">Supplement</a></strong> |
        <strong><a href="http://efrosprojects.eecs.berkeley.edu/averageExplorer/average_explorer.bib">BibTex</a></strong><br></p>
      </tr>
</tbody></table>


<p class="margin-small">&nbsp;</p>
<table border="0" >
  <tbody><tr>
    <td width="210"><img src="imgs/milcut.jpg" border="1" height = "140" width="210"></td>
    <td width="20"></td>
    <td width = "800"><p class="content"><strong>MILCut: A Sweeping Line  Multiple Instance Learning Paradigm for Interactive Image  Segmentation</strong></p>
      <p class="content"><a href="http://jiajunwu.com/">Jiajun Wu</a>*, <a href="http://www.stat.ucla.edu/~yibiao.zhao/">Yibiao Zhao</a>*, Jun-Yan Zhu, Siwei Luo and <a href="http://pages.ucsd.edu/~ztu/">Zhuowen Tu</a></p>
      <p class="content">In IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2014</p>

      <p class="margin-small">&nbsp;</p>
      <p class="content">
				 <strong><a href="http://jiajunwu.com/projects/milcut.html">Project </a></strong> |
				 <strong><a href="projects/MILCut/cvpr14_milcut.pdf">Paper </a></strong> |
				 <strong><a href="projects/MILCut/cvpr14_milcut_poster.pdf">Poster</a></strong> |
				 <strong><a href="projects/MILCut/cvpr14_milcut.bib">BibTex</a></strong><br></p></td>
  </tr>
</tbody></table>

<p class="margin-small">&nbsp;</p>
<table border="0" >
    <tbody> <tr>
      <td><img src="imgs/reverse_seg.jpg" border="1" height = "140" width="210"></td>
      <td width="20"></td>
      <td width="800"><p class="content"><strong>Reverse Image Segmentation: A High-Level Solution to a Low-Level Task</strong></p>
        <p class="content"><a href="http://jiajunwu.com/">Jiajun Wu</a>, Jun-Yan Zhu, and <a href="http://pages.ucsd.edu/~ztu/">Zhuowen Tu</a></p>
        <p class="content">In British Machine Vision Conference (<strong>BMVC</strong>), 2014</p>
        <p class="margin-small">&nbsp;</p>
      <p class="content">
				<strong><a href="projects/reverse_seg/reverse_seg.pdf">Paper</a></strong> |
				<strong><a href="projects/reverse_seg/reverse_seg.bib">BibTex</a></strong><br></p></td>
    </tr>
  </tbody>
</table>


<p class="margin-small">&nbsp;</p>
<table border="0" >
  <tbody><tr>
    <td width="140"><a href="projects/bMCL/"><img src="imgs/bmcl.jpg" border="1" height = "140" width="210"></td>
    <td width="20"></td>
    <td width="800"><p class="content"><strong>Unsupervised Object Class Discovery via Saliency-Guided Multiple Class Learning</strong></p>
      <p class="content">Jun-Yan Zhu, <a href="http://jiajunwu.com/">Jiajun Wu</a>, <a href="http://bme.buaa.edu.cn/teacherInfo.aspx?catID=7&subcatID=141&curID=355">Yan Xu</a>, <a href="http://research.microsoft.com/en-us/people/echang/">Eric Chang</a> and <a href="http://pages.ucsd.edu/~ztu/">Zhuowen Tu</a></p>
      <p class="content">In  IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), 2015</p>
      <p class="content">(an expanded journal version of our <strong>CVPR</strong> 2012 <strong><a href="projects/bMCL/cvpr12_bmcl.pdf">paper</a></strong>)</p>
      <p class="margin-small">&nbsp;</p>
      <p class="content"><strong><a href="projects/bMCL/index.html">Project</a></strong> |
			<strong><a href="projects/bMCL/pami_bmcl.pdf">Paper</a></strong> |
      <strong><a href="projects/bMCL/bMCL_supp.pdf">Supplement</a></strong> |
			<strong><a href="projects/bMCL/cvpr12_bmcl_poster.pdf">Poster</a></strong> |
			<strong><a href="projects/bMCL/pami_bmcl.bib">BibTex</a></strong><br> </p></td>
  </tr>
</tbody></table>

<p class="margin-small">&nbsp;</p>
<table border="0" >
  <tbody><tr>
    <td width="140"><a href="projects/MCIL/"><img src="imgs/mia14_mcil.jpg" border="1" height = "140" width="210"></td>
    <td width="20"></td>
    <td width="800"><p class="content"><strong>Multiple Clustered Instance Learning for Histopathology Cancer Image Classification, Segmentation and Clustering</strong></p>
      <p class="content"><a href="http://bme.buaa.edu.cn/teacherInfo.aspx?catID=7&subcatID=141&curID=355">Yan Xu</a>*, Jun-Yan Zhu*, <a href="http://research.microsoft.com/en-us/people/echang/">Eric I-Chao Chang</a> and <a href="http://pages.ucsd.edu/~ztu/">Zhuowen Tu</a></p>
      <p class="content">In IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2012</p>

      <p class="content">(See an expanded journal <strong><a href="projects/MCIL/mia14_mcil.pdf"> version</a></strong> at Medical Image Analysis (<strong>MIA</strong>), 2014</p>
      <p class="margin-small">&nbsp;</p>
      <p class="content"><strong><a href="projects/MCIL/index.html">Project</a></strong> |
      <strong><a href="https://github.com/junyanz/MCILBoost">GitHub</a></strong> |
			<strong><a href="projects/MCIL/cvpr12_mcil.pdf">Paper</a></strong> |
			<strong><a href="projects/MCIL/cvpr12_mcil.bib">BibTex</a></strong> |
			<strong> <a href="projects/MCIL/cvpr12_mcil_poster.pdf">Poster</a></strong><br></p></td>
  </tr>
</tbody></table>


<p class="margin-small">&nbsp;</p>
<table border="0">
  <tbody><tr>
    <td width="140"><img src="imgs/tip13_videoblend.gif" border="1" height="140" width="210"></td>
    <td width="20"></td>
    <td width="800"><p class="content"><strong>Motion-Aware Gradient Domain Video Composition</strong></p>
      <p class="content"><a href="http://www.ee.columbia.edu/ln/dvmm/vso/~taochen/">Tao Chen</a>, Jun-Yan Zhu, <a href="http://www.faculty.idc.ac.il/arik/site/index.asp">Ariel Shamir</a> and <a href="http://cg.cs.tsinghua.edu.cn/prof_hu.htm">Shi-Min Hu</a></p>
      <p class="content">In IEEE Transactions on Image Processing (<strong>TIP</strong>), 2013</p>
      <p class="margin-small">&nbsp;</p>
      <p class="content">
				<strong><a href="projects/videoblend/videoblend.pdf">Paper</a></strong> |
	  <strong><a href="http://youtu.be/AK2m4holVns">YouTube</a></strong> |
	  <strong><a href="projects/videoblend/videoblend.mp4">Video</a></strong> |
	  <strong> <a href="projects/videoblend//videoblend.bib">BibTex</a></strong><br></p></td>
  </tr>
</tbody></table>



<br>




<p id="sect-software" class="title-large">Software</p>
<p class="content"><strong><a href="https://github.com/NVIDIA/vid2vid">vid2vid</a></strong>: High-resolution (e.g., 2048x1024) photorealistic video-to-video translation.</p>
<p class="content"><strong><a href="https://github.com/jhoffman/cycada_release">CYCADA</a></strong>: Pytorch implementation of cycle-consistent adversarial domain adaptation.</p>
<p class="content"><strong><a href="https://github.com/NVIDIA/pix2pixHD">pix2pixHD</a></strong>: 2048x1024 image synthesis with conditional GANs.</p>
<p class="content"><strong><a href="https://github.com/junyanz/BicycleGAN">BicycleGAN</a></strong>: multimodal image-to-image translation.</p>
<p class="content"><strong><a href="https://github.com/junyanz/interactive-deep-colorization">Interactive Deep Colorization</a></strong>:  real-time interface for user-guided colorization.</p>
<p class="content"><strong><a href="https://github.com/richzhang/colorization-pytorch">PyTorch Colorization</a></strong>: PyTorch code for training interactive colorization models.</p>
<p class="content"><strong><a href="https://github.com/junyanz/light-field-video">Light Field Video</a></strong>: light field video applications (e.g. video refocusing, changing aperture and view).</p>
<p class="content"><strong><a href="https://github.com/junyanz/CycleGAN">CycleGAN</a></strong>: Torch implementation for learning an image-to-image translation without input-output pairs.</p>
<p class="content"><strong><a href="https://github.com/phillipi/pix2pix">pix2pix</a></strong>: Torch implementation for learning a mapping from input images to output images.</p>
<p class="content"><strong><a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix">pytorch CycleGAN & pix2pix</a></strong>: PyTorch implementation for both unpaired and paired image-to-image translation.</p>
<p class="content"><strong><a href="https://github.com/junyanz/iGAN">iGAN</a></strong>: a deep learning software that easily generates images with a few brushstrokes.</p>
<p class="content"><strong><a href="https://github.com/junyanz/RealismCNN">RealismCNN</a></strong>: code for predicting and improving visual realism in composite images.</p>
<p class="content"><strong><a href="https://github.com/junyanz/MCILBoost">MCILBoost</a></strong>: a boosting-based Multiple Instance Learning (MIL) software.</p>
<p class="content"><strong><a href="https://github.com/junyanz/MirrorMirror">MirrorMirror</a></strong>: an expression training App that helps users mimic their own expressions.</p>
<p class="content"><strong><a href="https://github.com/junyanz/SelectGoodFace">SelectGoodFace</a></strong>: a program for selecting attractive/serious portraits from a personal photo collection.</p>
<p class="content"><strong><a href="https://github.com/junyanz/FaceDemo">FaceDemo</a></strong>: a simple 3D face alignment and warping demo.</p>

<!--
<br>
<p id="sect-talks" class="title-large">Talks</p>
<p class="content"><strong><a href="https://youtu.be/MkluiD2lYCc?t=1h16m58s">Learning to Generate Images</a></strong></p>
<p class="content">SIGGRAPH Dissertation Award Talk (2018)</p>

<p class="content"><strong><a href="http://efrosgans.eecs.berkeley.edu/CVPR18_slides/CycleGAN.pptx">Unpaired Image-to-Image Translation</a></strong></p>
<p class="content">CVPR Tutorial on GANs (2018)</p>

<p class="content"><strong><a href="talks/talk_natural_photos.pptx">Learning to Synthesize and Manipulate Natural Photos</a></strong></p>
<p class="content">MIT CSAIL, HKUST CSE Departmental Seminar, ICCV Tutorial on GANs, O'Reilly AI, AI with the best, Y Conf, DEVIEW, ODSC West (2017)</p>

<p class="content"><strong><a href="talks/image_translation.pptx">On Image-to-Image Translation</a></strong></p>
<p class="content">Stanford, MIT CSAIL, Facebook, CUHK, SNU (2017)</p>

<p class="content"><strong><a href="talks/ideepcolor.pptx">Interactive Deep Colorization</a></strong></p>
<p class="content">SIGGRAPH, NVIDIA Innovation Theater, Global AI Hackathon (2017)</p>

<p class="content"><strong><a href="talks/manipulation_synthesis_junyanz.pptx">Visual Manipulation and Synthesis on the Natural Image Manifold</a></strong></p>
<p class="content">Facebook, MSR, Berkeley BAIR, THU, ICML workshop "Visualization for Deep Learning" (2016)</p>

<p class="content"><strong><a href="http://efrosprojects.eecs.berkeley.edu/mirrormirror/mirrormirror_slides.pptx">Mirror Mirror: Crowdsourcing Better Portraits</a></strong></p>
<p class="content">SIGGRAPH Asia (2014)</p>
<p class="content"><strong><a href="talks/siga14_course_ddvc_junyanz.pdf">What Makes Big Visual Data Hard?</a></strong></p>
<p class="content">SIGGRAPH Asia invited course &quot;Data-Driven Visual Computing&quot; (2014)</p>
<p class="content"><strong><a href="http://efrosprojects.eecs.berkeley.edu/averageExplorer/averageExplorer_slides.pptx">AverageExplorer: Interactive Exploration and Alignment of Visual Data Collections</a></strong></p>
<p class="content">SIGGRAPH (2014)</p>
<p class="content"><strong><a href="talks/wsl_slides.pptx">Discovering Objects and Harvesting Visual Concepts via Weakly Supervised Learning</a></strong></p>
<p class="content"> Berkeley Visual Computing Lab (2014)</p>
<br>
-->


<p id="sect-awards" class="title-large">Awards</p>
<p class="content">ACM SIGGRAPH Outstanding Doctoral Dissertation Award (2018)</p>
<p class="content">David J. Sakrison Memorial Prize for Outstanding Doctoral Research, by Berkeley EECS (2018)</p>
<p class="content">NVIDIA Pioneer Research Award (2018)</p>
<p class="content">Facebook Fellowship (2015)</p>
<p class="content">Outstanding Undergraduate Thesis in Tsinghua University (2012)</p>
<p class="content">Excellent Undergraduate Student in Tsinghua University (2012)</p>
<p class="content">National Scholarship, by the Ministry of Education of China (2009 and 2010)</p>
<p class="content">Singapore Technologies Engineering China Scholarship (2010, 2011, and 2012)</p>
<br>

<!-- <p id="sect-misc" class="title-large">MISC</p>
<p class="content"><strong><a href="imgs/pet.JPG">Photo</a></strong> of my cat Aquarius and my dog Arya.</p> -->

<div style="display:none">
<!-- GoStats JavaScript Based Code -->
<script type="text/javascript" src="http://gostats.com/js/counter.js"></script>
<script type="text/javascript">_gos='c3.gostats.com';_goa=390583;
_got=4;_goi=1;_goz=0;_god='hits';_gol='web page statistics from GoStats';_GoStatsRun();</script>
<!-- <a target="_blank" title="web page statistics from GoStats"
href="http://gostats.com"> -->
</div>
<!--
<img alt="web page statistics from GoStats"
src="http://c3.gostats.com/bin/count/a_390583/t_4/i_1/z_0/show_hits/counter.png"
style="border-width:0" />
</a>
-->
<!-- End GoStats JavaScript Based Code -->
</body></html>
